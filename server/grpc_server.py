import grpc
from concurrent import futures
from sentence_transformers import SentenceTransformer
import torch

from generated import embed_pb2, embed_pb2_grpc

# ëª¨ë¸ ë¡œë”©
MODEL_PATH = "/home/dev/embedding-server/KcELECTRA/models"  # ë„ˆê°€ í´ë¡ í•œ ê²½ë¡œ
device = "cuda:0" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer(MODEL_PATH, device=device)

# gRPC ì„œë¹„ìŠ¤ êµ¬í˜„
class EmbedderService(embed_pb2_grpc.EmbedderServicer):
    def GetEmbedding(self, request, context):
        texts = request.texts
        print(f"ğŸ’¬ ë°›ì€ ë¬¸ì¥ ê°œìˆ˜: {len(texts)}")

        # ì„ë² ë”© ìˆ˜í–‰
        embeddings = model.encode(texts, convert_to_numpy=True)
        dim = embeddings.shape[1]

        # ì‘ë‹µ ì¤€ë¹„
        vectors = []
        for emb in embeddings:
            vector = embed_pb2.Vector(values=emb.tolist())
            vectors.append(vector)

        return embed_pb2.EmbedResponse(
            vectors=vectors,
            dimension=dim,
            embeddings=embeddings.flatten().tolist()  # optional
        )

# ì„œë²„ ì‹¤í–‰
def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    embed_pb2_grpc.add_EmbedderServicer_to_server(EmbedderService(), server)
    server.add_insecure_port("[::]:50051")
    print("ğŸš€ gRPC ì„œë²„ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 50051)...")
    server.start()
    server.wait_for_termination()

if __name__ == "__main__":
    serve()
